{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "year - 1969, month - 6\n",
      "year - 1969, month - 7les = 8000\n",
      "year - 1969, month - 8les = 8000\n",
      "1969-8-processed articles = 8000\r"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "# from spacy import displacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from collections import Counter\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load(disable=['parser', 'tagger', 'textcat'])\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(depth=6)\n",
    "\n",
    "import codecs\n",
    "import os\n",
    "import urllib, json\n",
    "from __future__ import print_function\n",
    "\n",
    "start_year = 1969\n",
    "end_year = 1969\n",
    "\n",
    "start_month = 6\n",
    "end_month = 8\n",
    "\n",
    "for year in range(start_year,end_year+1):\n",
    "    for month in range(start_month,end_month+1):\n",
    "        filename = \"D:\\\\Gatech\\\\DVA\\\\News\\\\Data\\\\Input\\\\{}_{}.tsv\".format(year,month)\n",
    "        url = \"http://api.nytimes.com/svc/archive/v1/{}/{}.json?api-key=7ca882d86b844dfe852cb55040cbb0a7\".format(year,month)\n",
    "        response = urllib.urlopen(url)\n",
    "        data = json.loads(response.read().encode('utf-8'))\n",
    "        print(\"year - {}, month - {}\".format(year,month))\n",
    "\n",
    "        if not os.path.exists(os.path.dirname(filename)):\n",
    "            try:\n",
    "                os.makedirs(os.path.dirname(filename))\n",
    "            except OSError as exc: # Guard against race condition\n",
    "                if exc.errno != errno.EEXIST:\n",
    "                    raise\n",
    "\n",
    "        with codecs.open(filename, \"w\", \"utf-8\") as newsfile:\n",
    "            i = 0\n",
    "            for article in data['response']['docs']:\n",
    "                if '_id' in article and article['_id'] != None:\n",
    "                    id = article['_id']\n",
    "                else:\n",
    "                    id = \"\"\n",
    "\n",
    "                if \"pub_date\" in article and article['pub_date'] != None:\n",
    "                    pub_date = article['pub_date']\n",
    "                else:\n",
    "                    pub_date = \"\"\n",
    "\n",
    "                if 'headline' in article and article['headline'] != None:\n",
    "                    if 'main' in article['headline'] and article['headline']['main'] != None:\n",
    "                        headline = article['headline']['main']\n",
    "                    else:\n",
    "                        headline = unicode(\"\", \"utf-8\")\n",
    "                else:\n",
    "                    headline = unicode(\"\", \"utf-8\")\n",
    "\n",
    "\n",
    "                if 'snippet' in article and article['snippet'] != None:\n",
    "                    snippet = article['snippet']\n",
    "                else:\n",
    "                    snippet = unicode(\"\", \"utf-8\")\n",
    "\n",
    "                if 'multimedia' in article and len(article['multimedia']) != 0:\n",
    "                    if 'url' in article['multimedia'][0]:\n",
    "                        image = \"https://www.nytimes.com/\" + article['multimedia'][0]['url']\n",
    "                    else:\n",
    "                        image = \"\"\n",
    "                else:\n",
    "                    image = \"\"\n",
    "\n",
    "                if 'web_url' in article and article['web_url'] != None:\n",
    "                    web_url = article['web_url']\n",
    "                else:\n",
    "                    web_url = \"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                if 'keywords' in article and article['keywords'] != None:\n",
    "                    keywords = list(map(lambda x: x['value'],article['keywords']))\n",
    "                else:\n",
    "                    keywords = \"\"\n",
    "\n",
    "            #     if \"saudi\" in snippet.lower():\n",
    "            #         print(id + \" ---- \" +snippet)\n",
    "            #         doc = nlp(snippet)\n",
    "            #         pp.pprint([(X.text, X.label_) for X in doc.ents])\n",
    "            #         print(keywords)\n",
    "            #         print('-'*10)\n",
    "\n",
    "                word_list = []\n",
    "                doc = nlp(snippet)\n",
    "                [word_list.append(X.text) for X in doc.ents]\n",
    "                doc = nlp(headline)\n",
    "                [word_list.append(X.text) for X in doc.ents]\n",
    "            #     [word_list.append(x.lower()) for x in keywords]\n",
    "\n",
    "                for word in word_list:\n",
    "                    clean_headline = headline.replace(word,word.replace(\" \",\"~\"))\n",
    "                    clean_snippet = snippet.replace(word,word.replace(\" \",\"~\"))\n",
    "\n",
    "                grouped_keywords = list(map(lambda x: x.replace(\" \",\"~\"),keywords))\n",
    "\n",
    "                doc_keywords = \"\"\n",
    "                doc_keywords += clean_headline\n",
    "                doc_keywords += \" \"\n",
    "                doc_keywords += clean_snippet\n",
    "                doc_keywords += \" \"\n",
    "                doc_keywords += \" \".join(grouped_keywords)\n",
    "                doc_keywords = doc_keywords.replace(\",\",\"\")\n",
    "                doc_keywords = doc_keywords.replace(\";\",\"\")\n",
    "                doc_keywords = doc_keywords.replace(\":\",\"\")\n",
    "                doc_keywords = doc_keywords.replace(\".\",\"\")\n",
    "                doc_keywords = doc_keywords.replace('\"',\"\")\n",
    "                doc_keywords = doc_keywords.replace(\"-\",\"\")\n",
    "                doc_keywords = doc_keywords.replace(\"(\",\"\")\n",
    "                doc_keywords = doc_keywords.replace(\")\",\"\")\n",
    "                doc_keywords = doc_keywords.replace(\"'s\".decode('utf-8'),\"\")\n",
    "                doc_keywords = doc_keywords.replace(\"”\".decode('utf-8'),\"\")\n",
    "                doc_keywords = doc_keywords.replace(\"“\".decode('utf-8'),\"\")\n",
    "                doc_keywords = doc_keywords.replace(\"'\".decode('utf-8'),\"\")\n",
    "                doc_keywords = doc_keywords.replace(\"?\",\"\")\n",
    "                doc_keywords = doc_keywords.replace(\"’s\".decode('utf-8'),\"\")\n",
    "                doc_keywords = doc_keywords.replace(\"’ll\".decode('utf-8'),\"\")\n",
    "                doc_keywords = doc_keywords.replace(\"’\".decode('utf-8'),\"\")\n",
    "\n",
    "                STOP_WORDS.add(\"dont\")\n",
    "                STOP_WORDS.add(\"and\")\n",
    "            #     STOP_WORDS.add(\"your\")\n",
    "            #     STOP_WORDS.add(\"how\")\n",
    "            #     STOP_WORDS.add(\"which\")\n",
    "            #     STOP_WORDS.add(\"what\")\n",
    "            #     STOP_WORDS.add(\"when\")\n",
    "            #     STOP_WORDS.add(\"where\")\n",
    "            #     STOP_WORDS.add(\"why\")\n",
    "\n",
    "\n",
    "                cleaned_keywords_list = []\n",
    "                for word in doc_keywords.split(\" \"):\n",
    "                    if word.lower() not in STOP_WORDS:\n",
    "                        cleaned_keywords_list.append(word.lower())\n",
    "\n",
    "\n",
    "                cleaned_keywords = \" \".join(cleaned_keywords_list)\n",
    "                if headline != \"\":\n",
    "                    newsfile.write((id\n",
    "                          +\"\\t\"+\n",
    "                          pub_date\n",
    "                          +\"\\t\"+\n",
    "                          \"'\"+headline+\"'\"\n",
    "                          +\"\\t\"+\n",
    "                          \"'\"+snippet+\"'\"\n",
    "                          +\"\\t\"+\n",
    "                          web_url\n",
    "                          +\"\\t\"+\n",
    "                          image\n",
    "                          +\"\\t\"+\n",
    "                          cleaned_keywords).replace(\"\\n\",\" \"))\n",
    "                    newsfile.write(\"\\n\")\n",
    "\n",
    "                if i%1000 == 0:\n",
    "                    print(\"{}-{}-processed articles = \".format(year,month) + str(i),end=\"\\r\")\n",
    "                i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2017\n",
    "month = 2\n",
    "filename = \"D:\\\\Gatech\\\\DVA\\\\News\\\\Data\\\\{}_{}.tsv\".format(year,month)\n",
    "url = \"http://api.nytimes.com/svc/archive/v1/{}/{}.json?api-key=7ca882d86b844dfe852cb55040cbb0a7\".format(year,month)\n",
    "response = urllib.urlopen(url)\n",
    "data = json.loads(response.read().encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The New York Times film critics review “My Life as a Zucchini,” “Dying Laughing” and “Get Out.\"\n"
     ]
    }
   ],
   "source": [
    "for article in data['response']['docs']:\n",
    "    if \"58af512995d0e024746383d5\" == article[\"_id\"]:\n",
    "        print(article[\"snippet\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Analytics]",
   "language": "python",
   "name": "conda-env-Analytics-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
