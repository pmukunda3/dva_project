{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "year - 2016, month - 2\n",
      "2016-2-processed articles = 0\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-aa689a80da63>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     93\u001b[0m                 \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msnippet\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m                 \u001b[1;33m[\u001b[0m\u001b[0mword_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ments\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 95\u001b[1;33m                 \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mheadline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     96\u001b[0m                 \u001b[1;33m[\u001b[0m\u001b[0mword_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ments\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m             \u001b[1;31m#     [word_list.append(x.lower()) for x in keywords]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Akshay\\Anaconda3\\envs\\Analytics\\lib\\site-packages\\spacy\\language.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, text, disable)\u001b[0m\n\u001b[0;32m    344\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mproc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'__call__'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    345\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mE003\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcomponent\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mproc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 346\u001b[1;33m             \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mproc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    347\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    348\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mE005\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mnn_parser.pyx\u001b[0m in \u001b[0;36mspacy.syntax.nn_parser.Parser.__call__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mnn_parser.pyx\u001b[0m in \u001b[0;36mspacy.syntax.nn_parser.Parser.parse_batch\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mnn_parser.pyx\u001b[0m in \u001b[0;36mspacy.syntax.nn_parser.Parser.get_batch_model\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Akshay\\Anaconda3\\envs\\Analytics\\lib\\site-packages\\thinc\\api.pyc\u001b[0m in \u001b[0;36mbegin_update\u001b[1;34m(self, X, drop)\u001b[0m\n\u001b[0;32m     59\u001b[0m         \u001b[0mcallbacks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_layers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minc_layer_grad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbegin_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minc_layer_grad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mcontinue_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msgd\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Akshay\\Anaconda3\\envs\\Analytics\\lib\\site-packages\\thinc\\api.pyc\u001b[0m in \u001b[0;36mbegin_update\u001b[1;34m(seqs_in, drop)\u001b[0m\n\u001b[0;32m    278\u001b[0m         \u001b[0mlengths\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mseq\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mseqs_in\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    279\u001b[0m         X, bp_layer = layer.begin_update(layer.ops.flatten(seqs_in, pad=pad),\n\u001b[1;32m--> 280\u001b[1;33m                                          drop=drop)\n\u001b[0m\u001b[0;32m    281\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mbp_layer\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    282\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpad\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Akshay\\Anaconda3\\envs\\Analytics\\lib\\site-packages\\thinc\\api.pyc\u001b[0m in \u001b[0;36mbegin_update\u001b[1;34m(self, X, drop)\u001b[0m\n\u001b[0;32m     59\u001b[0m         \u001b[0mcallbacks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_layers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minc_layer_grad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbegin_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minc_layer_grad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mcontinue_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msgd\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Akshay\\Anaconda3\\envs\\Analytics\\lib\\site-packages\\thinc\\neural\\_classes\\resnet.pyc\u001b[0m in \u001b[0;36mbegin_update\u001b[1;34m(self, X, drop)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mbegin_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m         \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbp_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_layers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbegin_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mresidual_bwd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msgd\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Akshay\\Anaconda3\\envs\\Analytics\\lib\\site-packages\\thinc\\api.pyc\u001b[0m in \u001b[0;36mbegin_update\u001b[1;34m(self, X, drop)\u001b[0m\n\u001b[0;32m     59\u001b[0m         \u001b[0mcallbacks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_layers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minc_layer_grad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbegin_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minc_layer_grad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mcontinue_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msgd\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "# from spacy import displacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from collections import Counter\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load(disable=['parser', 'tagger', 'textcat'])\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(depth=6)\n",
    "\n",
    "import codecs\n",
    "import os\n",
    "import urllib, json\n",
    "from __future__ import print_function\n",
    "\n",
    "start_year = 2016\n",
    "end_year = 2016\n",
    "\n",
    "start_month = 2\n",
    "end_month = 2\n",
    "\n",
    "for year in range(start_year,end_year+1):\n",
    "    for month in range(start_month,end_month+1):\n",
    "        filename = \"D:\\\\Gatech\\\\DVA\\\\News\\\\Data\\\\{}_{}.tsv\".format(year,month)\n",
    "        url = \"http://api.nytimes.com/svc/archive/v1/{}/{}.json?api-key=7ca882d86b844dfe852cb55040cbb0a7\".format(year,month)\n",
    "        response = urllib.urlopen(url)\n",
    "        data = json.loads(response.read().encode('utf-8'))\n",
    "        print(\"year - {}, month - {}\".format(year,month))\n",
    "\n",
    "        if not os.path.exists(os.path.dirname(filename)):\n",
    "            try:\n",
    "                os.makedirs(os.path.dirname(filename))\n",
    "            except OSError as exc: # Guard against race condition\n",
    "                if exc.errno != errno.EEXIST:\n",
    "                    raise\n",
    "\n",
    "        with codecs.open(filename, \"w\", \"utf-8\") as newsfile:\n",
    "            i = 0\n",
    "            for article in data['response']['docs']:\n",
    "                if '_id' in article and article['_id'] != None:\n",
    "                    id = article['_id']\n",
    "                else:\n",
    "                    id = \"\"\n",
    "\n",
    "                if \"pub_date\" in article and article['pub_date'] != None:\n",
    "                    pub_date = article['pub_date']\n",
    "                else:\n",
    "                    pub_date = \"\"\n",
    "\n",
    "                if 'headline' in article and article['headline'] != None:\n",
    "                    if 'main' in article['headline'] and article['headline']['main'] != None:\n",
    "                        headline = article['headline']['main']\n",
    "                    else:\n",
    "                        headline = unicode(\"\", \"utf-8\")\n",
    "                else:\n",
    "                    headline = unicode(\"\", \"utf-8\")\n",
    "\n",
    "\n",
    "                if 'snippet' in article and article['snippet'] != None:\n",
    "                    snippet = article['snippet']\n",
    "                else:\n",
    "                    snippet = unicode(\"\", \"utf-8\")\n",
    "\n",
    "                if 'multimedia' in article and len(article['multimedia']) != 0:\n",
    "                    if 'url' in article['multimedia'][0]:\n",
    "                        image = \"https://www.nytimes.com/\" + article['multimedia'][0]['url']\n",
    "                    else:\n",
    "                        image = \"\"\n",
    "                else:\n",
    "                    image = \"\"\n",
    "\n",
    "                if 'web_url' in article and article['web_url'] != None:\n",
    "                    web_url = article['web_url']\n",
    "                else:\n",
    "                    web_url = \"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                if 'keywords' in article and article['keywords'] != None:\n",
    "                    keywords = list(map(lambda x: x['value'],article['keywords']))\n",
    "                else:\n",
    "                    keywords = \"\"\n",
    "\n",
    "            #     if \"saudi\" in snippet.lower():\n",
    "            #         print(id + \" ---- \" +snippet)\n",
    "            #         doc = nlp(snippet)\n",
    "            #         pp.pprint([(X.text, X.label_) for X in doc.ents])\n",
    "            #         print(keywords)\n",
    "            #         print('-'*10)\n",
    "\n",
    "                word_list = []\n",
    "                doc = nlp(snippet)\n",
    "                [word_list.append(X.text) for X in doc.ents]\n",
    "                doc = nlp(headline)\n",
    "                [word_list.append(X.text) for X in doc.ents]\n",
    "            #     [word_list.append(x.lower()) for x in keywords]\n",
    "\n",
    "                for word in word_list:\n",
    "                    clean_headline = headline.replace(word,word.replace(\" \",\"~\"))\n",
    "                    clean_snippet = snippet.replace(word,word.replace(\" \",\"~\"))\n",
    "\n",
    "                grouped_keywords = list(map(lambda x: x.replace(\" \",\"~\"),keywords))\n",
    "\n",
    "                doc_keywords = \"\"\n",
    "                doc_keywords += clean_headline\n",
    "                doc_keywords += \" \"\n",
    "                doc_keywords += clean_snippet\n",
    "                doc_keywords += \" \"\n",
    "                doc_keywords += \" \".join(grouped_keywords)\n",
    "                doc_keywords = doc_keywords.replace(\",\",\"\")\n",
    "                doc_keywords = doc_keywords.replace(\";\",\"\")\n",
    "                doc_keywords = doc_keywords.replace(\":\",\"\")\n",
    "                doc_keywords = doc_keywords.replace(\".\",\"\")\n",
    "                doc_keywords = doc_keywords.replace('\"',\"\")\n",
    "                doc_keywords = doc_keywords.replace(\"-\",\"\")\n",
    "                doc_keywords = doc_keywords.replace(\"(\",\"\")\n",
    "                doc_keywords = doc_keywords.replace(\")\",\"\")\n",
    "                doc_keywords = doc_keywords.replace(\"'s\".decode('utf-8'),\"\")\n",
    "                doc_keywords = doc_keywords.replace(\"”\".decode('utf-8'),\"\")\n",
    "                doc_keywords = doc_keywords.replace(\"“\".decode('utf-8'),\"\")\n",
    "                doc_keywords = doc_keywords.replace(\"'\".decode('utf-8'),\"\")\n",
    "                doc_keywords = doc_keywords.replace(\"?\",\"\")\n",
    "                doc_keywords = doc_keywords.replace(\"’s\".decode('utf-8'),\"\")\n",
    "                doc_keywords = doc_keywords.replace(\"’ll\".decode('utf-8'),\"\")\n",
    "                doc_keywords = doc_keywords.replace(\"’\".decode('utf-8'),\"\")\n",
    "\n",
    "                STOP_WORDS.add(\"dont\")\n",
    "                STOP_WORDS.add(\"and\")\n",
    "            #     STOP_WORDS.add(\"your\")\n",
    "            #     STOP_WORDS.add(\"how\")\n",
    "            #     STOP_WORDS.add(\"which\")\n",
    "            #     STOP_WORDS.add(\"what\")\n",
    "            #     STOP_WORDS.add(\"when\")\n",
    "            #     STOP_WORDS.add(\"where\")\n",
    "            #     STOP_WORDS.add(\"why\")\n",
    "\n",
    "\n",
    "                cleaned_keywords_list = []\n",
    "                for word in doc_keywords.split(\" \"):\n",
    "                    if word.lower() not in STOP_WORDS:\n",
    "                        cleaned_keywords_list.append(word.lower())\n",
    "\n",
    "\n",
    "                cleaned_keywords = \" \".join(cleaned_keywords_list)\n",
    "                if headline != \"\":\n",
    "                    newsfile.write((id\n",
    "                          +\"\\t\"+\n",
    "                          pub_date\n",
    "                          +\"\\t\"+\n",
    "                          headline\n",
    "                          +\"\\t\"+\n",
    "                          snippet\n",
    "                          +\"\\t\"+\n",
    "                          web_url\n",
    "                          +\"\\t\"+\n",
    "                          image\n",
    "                          +\"\\t\"+\n",
    "                          cleaned_keywords).replace(\"\\n\",\" \"))\n",
    "                    newsfile.write(\"\\n\")\n",
    "\n",
    "                if i%1000 == 0:\n",
    "                    print(\"{}-{}-processed articles = \".format(year,month) + str(i),end=\"\\r\")\n",
    "                i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2017\n",
    "month = 2\n",
    "filename = \"D:\\\\Gatech\\\\DVA\\\\News\\\\Data\\\\{}_{}.tsv\".format(year,month)\n",
    "url = \"http://api.nytimes.com/svc/archive/v1/{}/{}.json?api-key=7ca882d86b844dfe852cb55040cbb0a7\".format(year,month)\n",
    "response = urllib.urlopen(url)\n",
    "data = json.loads(response.read().encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The New York Times film critics review “The Space Between Us,” “The Comedian” and “I Am Not Your Negro.\"\n"
     ]
    }
   ],
   "source": [
    "for article in data['response']['docs']:\n",
    "    if \"5893bc1195d0e0392607e53a\" == article[\"_id\"]:\n",
    "        print(article[\"snippet\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Analytics]",
   "language": "python",
   "name": "conda-env-Analytics-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
