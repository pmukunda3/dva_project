{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "spacy.prefer_gpu()\n",
    "from spacy import displacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from collections import Counter\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load(disable=['parser', 'tagger', 'textcat'])\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(depth=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import urllib, json\n",
    "# url = \"http://api.nytimes.com/svc/archive/v1/2018/10.json?api-key=7ca882d86b844dfe852cb55040cbb0a7\"\n",
    "# response = urllib.request.urlopen(url)\n",
    "# data = json.loads(response.read())\n",
    "\n",
    "import urllib, json\n",
    "url = \"http://api.nytimes.com/svc/archive/v1/2016/10.json?api-key=7ca882d86b844dfe852cb55040cbb0a7\"\n",
    "response = urllib.urlopen(url)\n",
    "data = json.loads(response.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-d3bbc701976c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0murl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"http://api.nytimes.com/svc/archive/v1/{}/{}.json?api-key=7ca882d86b844dfe852cb55040cbb0a7\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myear\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmonth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0murllib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m         \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"year - {}, month - {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myear\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmonth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Akshay\\Anaconda3\\envs\\Analytics\\lib\\socket.pyc\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, size)\u001b[0m\n\u001b[0;32m    353\u001b[0m             \u001b[1;32mwhile\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    354\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 355\u001b[1;33m                     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrbufsize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    356\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    357\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mEINTR\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "spacy.prefer_gpu()\n",
    "from spacy import displacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from collections import Counter\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load(disable=['parser', 'tagger', 'textcat'])\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(depth=6)\n",
    "\n",
    "import codecs\n",
    "import os\n",
    "import urllib, json\n",
    "from __future__ import print_function\n",
    "\n",
    "start_year = 2016\n",
    "end_year = 2017\n",
    "\n",
    "start_month = 1\n",
    "end_month = 12\n",
    "\n",
    "for year in range(start_year,end_year+1):\n",
    "    for month in range(start_month,end_month+1):\n",
    "        filename = \"D:\\\\Gatech\\\\DVA\\\\News\\\\Data\\\\{}_{}.tsv\".format(year,month)\n",
    "        url = \"http://api.nytimes.com/svc/archive/v1/{}/{}.json?api-key=7ca882d86b844dfe852cb55040cbb0a7\".format(year,month)\n",
    "        response = urllib.urlopen(url)\n",
    "        data = json.loads(response.read())\n",
    "        print(\"year - {}, month - {}\".format(year,month))\n",
    "\n",
    "        if not os.path.exists(os.path.dirname(filename)):\n",
    "            try:\n",
    "                os.makedirs(os.path.dirname(filename))\n",
    "            except OSError as exc: # Guard against race condition\n",
    "                if exc.errno != errno.EEXIST:\n",
    "                    raise\n",
    "\n",
    "        with codecs.open(filename, \"w\", \"utf-8\") as newsfile:\n",
    "            i = 0\n",
    "            for article in data['response']['docs']:\n",
    "                if '_id' in article and article['_id'] != None:\n",
    "                    id = article['_id']\n",
    "                else:\n",
    "                    id = \"\"\n",
    "\n",
    "                if \"pub_date\" in article and article['pub_date'] != None:\n",
    "                    pub_date = article['pub_date']\n",
    "                else:\n",
    "                    pub_date = \"\"\n",
    "\n",
    "                if 'headline' in article and article['headline'] != None:\n",
    "                    if 'main' in article['headline'] and article['headline']['main'] != None:\n",
    "                        headline = article['headline']['main']\n",
    "                    else:\n",
    "                        headline = \"\"\n",
    "                else:\n",
    "                    headline = \"\"\n",
    "\n",
    "\n",
    "                if 'snippet' in article and article['snippet'] != None:\n",
    "                    snippet = article['snippet']\n",
    "                else:\n",
    "                    snippet = \"\"\n",
    "\n",
    "                if 'multimedia' in article and len(article['multimedia']) != 0:\n",
    "                    if 'url' in article['multimedia'][0]:\n",
    "                        image = \"https://www.nytimes.com/\" + article['multimedia'][0]['url']\n",
    "                    else:\n",
    "                        image = \"\"\n",
    "                else:\n",
    "                    image = \"\"\n",
    "\n",
    "                if 'web_url' in article and article['web_url'] != None:\n",
    "                    web_url = article['web_url']\n",
    "                else:\n",
    "                    web_url = \"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                if 'keywords' in article and article['keywords'] != None:\n",
    "                    keywords = list(map(lambda x: x['value'],article['keywords']))\n",
    "                else:\n",
    "                    keywords = \"\"\n",
    "\n",
    "            #     if \"saudi\" in snippet.lower():\n",
    "            #         print(id + \" ---- \" +snippet)\n",
    "            #         doc = nlp(snippet)\n",
    "            #         pp.pprint([(X.text, X.label_) for X in doc.ents])\n",
    "            #         print(keywords)\n",
    "            #         print('-'*10)\n",
    "\n",
    "                word_list = []\n",
    "                doc = nlp(snippet)\n",
    "                [word_list.append(X.text) for X in doc.ents]\n",
    "                doc = nlp(headline)\n",
    "                [word_list.append(X.text) for X in doc.ents]\n",
    "            #     [word_list.append(x.lower()) for x in keywords]\n",
    "\n",
    "                for word in word_list:\n",
    "                    clean_headline = headline.replace(word,word.replace(\" \",\"~\"))\n",
    "                    clean_snippet = snippet.replace(word,word.replace(\" \",\"~\"))\n",
    "\n",
    "                grouped_keywords = list(map(lambda x: x.replace(\" \",\"~\"),keywords))\n",
    "\n",
    "                doc_keywords = \"\"\n",
    "                doc_keywords += clean_headline\n",
    "                doc_keywords += \" \"\n",
    "                doc_keywords += clean_snippet\n",
    "                doc_keywords += \" \"\n",
    "                doc_keywords += \" \".join(grouped_keywords)\n",
    "                doc_keywords = doc_keywords.replace(\",\",\"\")\n",
    "                doc_keywords = doc_keywords.replace(\";\",\"\")\n",
    "                doc_keywords = doc_keywords.replace(\":\",\"\")\n",
    "                doc_keywords = doc_keywords.replace(\".\",\"\")\n",
    "                doc_keywords = doc_keywords.replace('\"',\"\")\n",
    "                doc_keywords = doc_keywords.replace(\"-\",\"\")\n",
    "                doc_keywords = doc_keywords.replace(\"(\",\"\")\n",
    "                doc_keywords = doc_keywords.replace(\")\",\"\")\n",
    "                doc_keywords = doc_keywords.replace(\"'s\".decode('utf-8'),\"\")\n",
    "                doc_keywords = doc_keywords.replace(\"'\".decode('utf-8'),\"\")\n",
    "                doc_keywords = doc_keywords.replace(\"?\",\"\")\n",
    "                doc_keywords = doc_keywords.replace(\"’s\".decode('utf-8'),\"\")\n",
    "                doc_keywords = doc_keywords.replace(\"’ll\".decode('utf-8'),\"\")\n",
    "                doc_keywords = doc_keywords.replace(\"’\".decode('utf-8'),\"\")\n",
    "\n",
    "                STOP_WORDS.add(\"dont\")\n",
    "                STOP_WORDS.add(\"and\")\n",
    "            #     STOP_WORDS.add(\"your\")\n",
    "            #     STOP_WORDS.add(\"how\")\n",
    "            #     STOP_WORDS.add(\"which\")\n",
    "            #     STOP_WORDS.add(\"what\")\n",
    "            #     STOP_WORDS.add(\"when\")\n",
    "            #     STOP_WORDS.add(\"where\")\n",
    "            #     STOP_WORDS.add(\"why\")\n",
    "\n",
    "\n",
    "                cleaned_keywords_list = []\n",
    "                for word in doc_keywords.split(\" \"):\n",
    "                    if word.lower() not in STOP_WORDS:\n",
    "                        cleaned_keywords_list.append(word.lower())\n",
    "\n",
    "\n",
    "                cleaned_keywords = \" \".join(cleaned_keywords_list)\n",
    "                if headline != \"\":\n",
    "                    newsfile.write(id\n",
    "                          +\"\\t\"+\n",
    "                          pub_date\n",
    "                          +\"\\t\"+\n",
    "                          headline\n",
    "                          +\"\\t\"+\n",
    "                          snippet\n",
    "                          +\"\\t\"+\n",
    "                          web_url\n",
    "                          +\"\\t\"+\n",
    "                          image\n",
    "                          +\"\\t\"+\n",
    "                          cleaned_keywords)\n",
    "                    newsfile.write(\"\\n\")\n",
    "\n",
    "                if i%1000 == 0:\n",
    "                    print(\"{}-{}processed articles = \".format(year,month) + str(i),end=\"\\r\")\n",
    "                i+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "year - 2016, month - 1\n"
     ]
    }
   ],
   "source": [
    "year = 2016\n",
    "month = 1\n",
    "url = \"http://api.nytimes.com/svc/archive/v1/{}/{}.json?api-key=7ca882d86b844dfe852cb55040cbb0a7\".format(year,month)\n",
    "response = urllib.urlopen(url)\n",
    "data = json.loads(response.read())\n",
    "print(\"year - {}, month - {}\".format(year,month))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "\n",
    "with codecs.open(\"news_spark.tsv\", \"w\", \"utf-8\") as newsfile:\n",
    "    for article in data['response']['docs']:\n",
    "        if '_id' in article and article['_id'] != None:\n",
    "            id = article['_id']\n",
    "        else:\n",
    "            id = \"\"\n",
    "\n",
    "        if 'headline' in article and 'main' in article['headline'] and article['headline']['main'] != None:\n",
    "            headline = article['headline']['main']\n",
    "        else:\n",
    "            headline = \"\"\n",
    "        if 'snippet' in article and article['snippet'] != None:\n",
    "                snippet = article['snippet']\n",
    "        else:\n",
    "            snippet = \"\"\n",
    "        if 'keywords' in article and article['keywords'] != None:\n",
    "                keywords = list(map(lambda x: x['value'],article['keywords']))\n",
    "        else:\n",
    "            keywords = \"\"\n",
    "        if id != \"\":   \n",
    "            newsfile.write(id+\"\\t\"+snippet+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(map(lambda x: x['value'],data['response']['docs'][0]['keywords']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['response']['docs'][0]['keywords']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select token, sum(tf_idf) from newsdata group by token order by sum(tf_idf) desc limit 400;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for article in data['response']['docs']:\n",
    "    if '_id' in article and article['_id'] != None:\n",
    "        id = article['_id']\n",
    "    else:\n",
    "        id = \"\"\n",
    "\n",
    "    if 'headline' in article and 'main' in article['headline'] and article['headline']['main'] != None:\n",
    "        headline = article['headline']['main']\n",
    "    else:\n",
    "        headline = \"\"\n",
    "    if 'snippet' in article and article['snippet'] != None:\n",
    "            snippet = article['snippet']\n",
    "    else:\n",
    "        snippet = \"\"\n",
    "    if 'keywords' in article and article['keywords'] != None:\n",
    "            keywords = list(map(lambda x: x['value'],article['keywords']))\n",
    "    else:\n",
    "        keywords = \"\"\n",
    "    \n",
    "    if \"saudi\" in snippet.lower():\n",
    "        print(id + \" ---- \" +snippet)\n",
    "        doc = nlp(snippet)\n",
    "        pp.pprint([(X.text, X.label_) for X in doc.ents])\n",
    "        print(keywords)\n",
    "        print('-'*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# import csv\n",
    "\n",
    "# with open('/Users/akshay/Downloads/tables_news_output_part-00000-tid-3696992905842582427-00003e92-ce81-4c48-bc2c-aa5e225a3bde-2754-c000.csv') as csv_file:\n",
    "#     csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "#     line_count = 0\n",
    "#     for row in csv_reader:\n",
    "#         print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Analytics]",
   "language": "python",
   "name": "conda-env-Analytics-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
